# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import (Dict, Optional)
from omegaconf import DictConfig

from cosyvoice.utils.mask import make_pad_mask


class MaskedDiffWithXvec(nn.Module):
    def __init__(self,
                 input_size: int = 512,
                 output_size: int = 80,
                 spk_embed_dim: int = 192,
                 output_type: str = "mel",
                 vocab_size: int = 4096,
                 input_frame_rate: int = 50,
                 only_mask_loss: bool = True,
                 encoder: torch.nn.Module = None,  # ConformerEncoder
                 length_regulator: torch.nn.Module = None,  # InterpolateRegulator
                 decoder: torch.nn.Module = None,  # ConditionalCFM
                 decoder_conf: Dict = {'in_channels': 240,
                                       'out_channel': 80,
                                       'spk_emb_dim': 80,
                                       'n_spks': 1,
                                       'cfm_params': DictConfig({'sigma_min': 1e-06,
                                                                 'solver': 'euler',
                                                                 't_scheduler': 'cosine',
                                                                 'training_cfg_rate': 0.2,
                                                                 'inference_cfg_rate': 0.7,
                                                                 'reg_loss_type': 'l1'}),
                                       'decoder_params': {'channels': [256, 256],
                                                          'dropout': 0.0,
                                                          'attention_head_dim': 64,
                                                          'n_blocks': 4,
                                                          'num_mid_blocks': 12,
                                                          'num_heads': 8,
                                                          'act_fn': 'gelu'}
                                       }):
        super().__init__()

        self.input_size = input_size  # 512
        self.output_size = output_size  # 80
        self.decoder_conf = decoder_conf
        self.vocab_size = vocab_size  # 4096
        self.output_type = output_type  # "mel"
        self.input_frame_rate = input_frame_rate  # 50
        logging.info(f"input frame rate={self.input_frame_rate}")
        self.input_embedding = nn.Embedding(vocab_size, input_size)  # 4096,512
        self.spk_embed_affine_layer = nn.Linear(
            spk_embed_dim, output_size)  # 192,80
        self.encoder = encoder  # ConformerEncoder
        self.encoder_proj = nn.Linear(
            self.encoder.output_size(), output_size)  # 512,80
        self.decoder = decoder  # ConditionalCFM
        self.length_regulator = length_regulator  # InterpolateRegulator
        self.only_mask_loss = only_mask_loss  # True

    def forward(self,
                batch: dict,
                device: torch.device) -> Dict[str, Optional[torch.Tensor]]:
        token = batch["speech_token"].to(device)
        token_len = batch["speech_token_len"].to(device)
        feat = batch["speech_feat"].to(device)
        feat_len = batch["speech_feat_len"].to(device)
        embedding = batch["embedding"].to(device)

        # xvec projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)

        # concat text and prompt_text
        mask = (~make_pad_mask(token_len)).float().unsqueeze(-1).to(device)
        token = self.input_embedding(torch.clamp(token, min=0)) * mask

        # text encode
        h, h_lengths = self.encoder(token, token_len)
        h = self.encoder_proj(h)
        h, h_lengths = self.length_regulator(h, feat_len)

        # get conditions
        conds = torch.zeros(feat.size(), device=token.device)
        for i, j in enumerate(feat_len):
            if random.random() < 0.5:
                continue
            index = random.randint(0, int(0.3 * j))
            conds[i, :index] = feat[i, :index]
        conds = conds.transpose(1, 2)

        mask(~make_pad_mask(feat_len)).to(h)
        # NOTE this is unnecessary, feat/h already same shape
        loss, _ = self.decoder.compute_loss(feat.transpose(1, 2).contiguous(),
                                            mask.unsqueeze(1),
                                            h.transpose(1, 2).contiguous(),
                                            embedding,
                                            cond=conds)
        return {"loss": loss}

    @torch.inference_mode()
    def inference(self,
                  token,  # [1,T_token] generated token from LLM
                  token_len,  # [1]=T_token
                  prompt_token,  # [1,0], prompt speech token
                  prompt_token_len,  # [1]=0
                  prompt_feat,  # [1,0,80]=0, prompt speech mel-feat
                  prompt_feat_len,  # [1]=0
                  embedding,  # [1,192], spk embedding
                  flow_cache):  # [1,80,0,2] all 0 is fake
        assert token.size(0) == 1
        # xvec projection
        embedding = F.normalize(embedding, dim=1)  # [1,192]
        embedding = self.spk_embed_affine_layer(embedding)  # [1,80]

        # concat speech token and prompt speech token
        token_len1, token_len2 = prompt_token.size(1), token.size(1)  # 0,120
        token, token_len = torch.concat(
            [prompt_token, token], dim=1), prompt_token_len + token_len
        mask = (~make_pad_mask(token_len)
                ).unsqueeze(-1).to(embedding)  # [1,120,1], float
        # [1,T_token,512], figure1 b 中的embedding
        token = self.input_embedding(torch.clamp(token, min=0)) * mask

        # text encode
        # [B,T_token,512],[1,1,T_token]
        h, h_lengths = self.encoder(token, token_len)
        h = self.encoder_proj(h)  # [B,T_token,80]
        mel_len1, mel_len2 = prompt_feat.shape[1], int(
            token_len2 / self.input_frame_rate * 22050 / 256)  # 0, 206
        h, h_lengths = self.length_regulator.inference(
            h[:, :token_len1], h[:, token_len1:], mel_len1, mel_len2, self.input_frame_rate)  # [1, mel_len1+mel_len2, 80], mel_len1+mel_len2=206

        # get conditions
        conds = torch.zeros(
            [1, mel_len1 + mel_len2, self.output_size], device=token.device).to(h.dtype)  # [1, 206, 80]
        conds[:, :mel_len1] = prompt_feat
        conds = conds.transpose(1, 2)  # [1, 80,mel_len1+mel_len2]

        # [1, mel_len1+mel_len2]
        mask = (~make_pad_mask(torch.tensor([mel_len1 + mel_len2]))).to(h)
        # next line corresonds to the  Fig1.C in the paper
        # conditions formed like <v|mu|x_mask|x_t>
        # mu is semantic tokens, concat prompt_token and llm generated token,
        # x_mask is masked speech feat,
        # x_t is intermediate state at timestep t.
        feat, flow_cache = self.decoder(mu=h.transpose(1, 2).contiguous(),
                                        mask=mask.unsqueeze(1),
                                        spks=embedding,
                                        cond=conds,
                                        n_timesteps=10,
                                        prompt_len=mel_len1,
                                        cache=flow_cache)  # flow_cache=[1,80,34,2]
        feat = feat[:, :, mel_len1:]  # [1,80,206]
        assert feat.shape[2] == mel_len2
        return feat.float(), flow_cache
